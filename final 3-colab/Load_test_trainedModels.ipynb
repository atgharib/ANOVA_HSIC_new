{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLXtaoZ6klOC",
        "outputId": "c281610b-d071-4816-f4ea-bb52216012ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/My\\ Drive/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7-Uk9oyjnHGF",
        "outputId": "ae88a08c-c85f-477d-a505-850e05bf081a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1.PNG\n",
            " 1.PNG.gdoc\n",
            "'Atieh Gharib-CV (1).pdf'\n",
            "'Atieh Gharib-CV (2).pdf'\n",
            "'Atieh Gharib-CV.pdf'\n",
            " bert_summarization\n",
            " books\n",
            "'Colab Notebooks'\n",
            "'Copy of Glenn%20Neely%20-%20Mastering%20Elliott%20Wave.pdf'\n",
            "'Copy of HSIC.zip'\n",
            " data_structures1-real1.rar\n",
            " data_structures1-real.rar\n",
            " data_structures.rar\n",
            "'Digital Dollar Sign PowerPoint Templates.pptx'\n",
            " docker_images\n",
            "'elliott pres2.pptx'\n",
            " elliott-wave-pattern2.rar\n",
            " EX07\n",
            " EX14.ipynb\n",
            " Forex_codes\n",
            "'Gharib Atieh-CV.pdf'\n",
            " GISPython.pdf\n",
            " Glenn%20Neely%20-%20Mastering%20Elliott%20Wave.pdf\n",
            " HSIC\n",
            " hsicfeaturegumbelsparsemax_autos.pkl\n",
            " HSICNet\n",
            " HSIC.zip\n",
            " Inverse_Problem_of_Ultrasound_Beamforming_With_Denoising-Based_Regularized_Solutions.pdf\n",
            " learning\n",
            "'Letter of employment, Atieh Gharib, special consulant 1.1.2025 - DTU Biosustain.pdf'\n",
            "'New Doc 2020-03-12 16.06.20.gdoc'\n",
            "'New Doc 2020-03-12 16.06.20.pdf'\n",
            " ONSPEED-DE-UDP.ovpn\n",
            " ONSPEED-FR-UDP.ovpn\n",
            " Openvpn\n",
            "'Proposal Neth Calls'\n",
            "'published data'\n",
            "'published data.zip'\n",
            " sherkat\n",
            "'Sworn declaration AR6.pdf'\n",
            " Untitled0.ipynb\n",
            " website_video.mp4\n",
            "'Well-known application.pptx'\n",
            " zohre.rar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/HSIC/\n",
        "!ls  # Lists files in the current folder\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBG7h4FtqPO4",
        "outputId": "e39fa328-11e5-4d2e-d86b-798522eb22f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/HSIC\n",
            "consistency_scores.csv\tfeature_importances_new.pkl  __pycache__       selected_features_new.pkl\n",
            "data\t\t\tHSICNet\t\t\t     real_datasets.py  trained_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sparsemax\n",
        "!pip install ucimlrepo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oKLVqTpiqdYZ",
        "outputId": "e81903cb-22de-435d-c29b-4bf9e9bf2c0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparsemax\n",
            "  Downloading sparsemax-0.1.9-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from sparsemax) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->sparsemax) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->sparsemax) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->sparsemax) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->sparsemax) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->sparsemax) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->sparsemax) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->sparsemax) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->sparsemax) (3.0.2)\n",
            "Downloading sparsemax-0.1.9-py2.py3-none-any.whl (5.5 kB)\n",
            "Installing collected packages: sparsemax\n",
            "Successfully installed sparsemax-0.1.9\n",
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from real_datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min, pairwise_distances\n",
        "import pickle\n",
        "import numpy as np\n",
        "from openpyxl import Workbook\n",
        "from real_datasets import load_dataset\n",
        "from HSICNet.HSICFeatureNet import *\n",
        "from HSICNet.HSICNet import *\n",
        "from HSICNet.util import *\n",
        "import gc\n",
        "import torch\n",
        "\n"
      ],
      "metadata": {
        "id": "yhAFaQtr3bIM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models(dataset_name, X_tensor_tbx):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Applying HSICFeatureNetGumbelSparsemax on dataset: {dataset_name}\")\n",
        "    # with open('trained_models/hsicfeaturegumbelsparsemax_autos.pkl', 'rb') as f:\n",
        "    with open('trained_models/hsicfeaturegumbelsparsemax_bike.pkl', 'rb') as f:\n",
        "        # featuregumbelsparsemax_model = torch.load(f, map_location=torch.device('cpu'))\n",
        "        # featuregumbelsparsemax_model = torch.load(f, map_location=device)\n",
        "        featuregumbelsparsemax_model = pickle.load(f)\n",
        "    featuregumbelsparsemax_model.eval()\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        hsic_fNET_gsp_weights, _ ,_ , _= featuregumbelsparsemax_model(X_tensor_tbx)\n",
        "    HSICFGSP_selected_features = (hsic_fNET_gsp_weights > 1e-3).to(torch.int32)\n",
        "\n",
        "\n",
        "    print(f\"Applying HSICGumbelSparsemax on dataset: {dataset_name}\")\n",
        "    # with open('trained_models/hsicnetgumbelsparsemax_autos.pkl', 'rb') as f:\n",
        "    with open('trained_models/hsicnetgumbelsparsemax_bike.pkl', 'rb') as f:\n",
        "        gumbelsparsemax_model = pickle.load(f)\n",
        "    gumbelsparsemax_model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "            hsic_gsp_weights, _ ,_ = gumbelsparsemax_model(X_tensor_tbx)\n",
        "    gumbelsparsemax_selected_features = (hsic_gsp_weights > 1e-3).to(torch.int32)\n",
        "\n",
        "    # model_filename = f\"trained_models/hsicnetgumbelsparsemax_{dataset_name}.pkl\"\n",
        "\n",
        "    feature_importances_new = [hsic_gsp_weights, hsic_fNET_gsp_weights]\n",
        "    selected_features_new = [gumbelsparsemax_selected_features, HSICFGSP_selected_features]\n",
        "    # Save feature_importances to a .pkl file\n",
        "    with open('feature_importances_new.pkl', 'wb') as f:\n",
        "            pickle.dump(feature_importances_new, f)\n",
        "\n",
        "    with open('selected_features_new.pkl', 'wb') as f:\n",
        "        pickle.dump(selected_features_new, f)"
      ],
      "metadata": {
        "id": "4vdeJesDrhJ6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "348cara3sCi3",
        "outputId": "95db0e93-8779-4d2a-e064-cff86e29674e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/HSIC\n",
            "consistency_scores.csv\tfeature_importances_new.pkl  __pycache__       selected_features_new.pkl\n",
            "data\t\t\tHSICNet\t\t\t     real_datasets.py  trained_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def consistency_across_methods(feature_matrix, feature_importances_list, method_names, n_clusters=5):\n",
        "    \"\"\"\n",
        "    Evaluate the consistency of feature importance values across similar (clustered) instances\n",
        "    for multiple attribution methods.\n",
        "\n",
        "    Parameters:\n",
        "        feature_matrix (numpy.ndarray): Test data feature matrix of shape [n_instances, n_features].\n",
        "                                         This represents input instances in feature space.\n",
        "        feature_importances_list (list of numpy.ndarray): A list of feature importance matrices.\n",
        "                                                          Each matrix corresponds to one method and has shape [n_instances, n_features].\n",
        "        method_names (list of str): Names of the methods corresponding to the feature importance matrices.\n",
        "        n_clusters (int): Number of clusters to group similar instances in feature space.\n",
        "\n",
        "    Returns:\n",
        "        consistency_scores (dict): A dictionary with method names as keys and their consistency scores\n",
        "                                    (list of float scores for each cluster) as values.\n",
        "    \"\"\"\n",
        "    feature_matrix = np.array(feature_matrix)  # Ensure feature_matrix is a NumPy array\n",
        "    consistency_scores = {}\n",
        "\n",
        "    for method_idx, (method_name, feature_importances) in enumerate(zip(method_names, feature_importances_list)):\n",
        "        print(f\"Evaluating consistency for method: {method_name}...\")\n",
        "        # feature_importances = np.array(feature_importances)  # Ensure importance matrix is a NumPy array\n",
        "        feature_importances = feature_importances.cpu().numpy()\n",
        "        # Perform clustering using K-Means\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(feature_matrix)\n",
        "\n",
        "        # Measure consistency within each cluster\n",
        "        cluster_consistency = []\n",
        "\n",
        "        for cluster_idx in range(n_clusters):\n",
        "            # Indices of instances in the current cluster\n",
        "            cluster_indices = np.where(cluster_labels == cluster_idx)[0]\n",
        "            cluster_features = feature_matrix[cluster_indices]  # Features of instances in the cluster\n",
        "            cluster_importances = feature_importances[cluster_indices]  # Importances of instances in the cluster\n",
        "\n",
        "            # If the cluster has just one element, skip it\n",
        "            if len(cluster_indices) <= 1:\n",
        "                cluster_consistency.append(0)  # Small cluster, by definition consistent\n",
        "                continue\n",
        "\n",
        "            # 1. Compute pairwise distances between instances in the cluster (feature space)\n",
        "            pairwise_feature_distances = pairwise_distances(cluster_features, metric='euclidean')\n",
        "\n",
        "            # 2. Compute pairwise distances between feature importance vectors (importance space)\n",
        "            pairwise_importance_distances = pairwise_distances(cluster_importances, metric='euclidean')\n",
        "\n",
        "            # 3. Normalize importance distances by feature space distances (consistency measure)\n",
        "            # Avoid division by zero in distances\n",
        "            normalized_distances = np.divide(\n",
        "                pairwise_importance_distances,\n",
        "                pairwise_feature_distances + 1e-8  # Small epsilon to prevent division by zero\n",
        "            )\n",
        "\n",
        "            # Use mean normalized pairwise distance as the consistency score for this cluster\n",
        "            cluster_consistency_score = np.mean(normalized_distances)\n",
        "            cluster_consistency.append(cluster_consistency_score)\n",
        "\n",
        "        # Store the consistency results for the current method\n",
        "        consistency_scores[method_name] = cluster_consistency\n",
        "\n",
        "    return consistency_scores"
      ],
      "metadata": {
        "id": "N482TMEtrsXZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_consistency_scores_to_csv(consistency_scores, dataset_name, filename=\"consistency_scores.csv\"):\n",
        "    \"\"\"\n",
        "    Save consistency scores to a CSV file, including the dataset name.\n",
        "\n",
        "    Parameters:\n",
        "        consistency_scores (dict): A dictionary where keys are method names and values are lists of\n",
        "                                    consistency scores for each cluster.\n",
        "        dataset_name (str): The name of the dataset.\n",
        "        filename (str): The name of the CSV file to save the results in.\n",
        "    \"\"\"\n",
        "    # Prepare data for CSV\n",
        "    data = []\n",
        "    for method_name, scores in consistency_scores.items():\n",
        "        for cluster_idx, score in enumerate(scores):\n",
        "            # Create a row for each (method, cluster, score)\n",
        "            data.append({\n",
        "                \"Dataset\": dataset_name,\n",
        "                \"Method\": method_name,\n",
        "                \"Cluster\": cluster_idx + 1,\n",
        "                \"Consistency Score\": score\n",
        "            })\n",
        "\n",
        "    # Convert to pandas DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save to a CSV file\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"Consistency scores saved to {filename}.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nJsIHkcA0vJv"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(torch.cuda.is_available())\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    method_names = [\n",
        "            'Hsic_GumbelSparsemax',\n",
        "            'HSICFeatureNet_GumbelSparsemax'\n",
        "\n",
        "        ]\n",
        "    # dataset_names = [\"breast_cancer\", \"sonar\", \"nomao\", \"steel\", \"breast_cancer_wisconsin\", \"skillcraft\", \"ionosphere\", \"sml\", \"pol\", \\\n",
        "                        #  'parkinson', 'keggdirected', \"pumadyn32nm\", \"crime\", \"gas\", 'autos', 'bike', 'keggundirected']\n",
        "    datasets= ['bike']\n",
        "        # Main running part of the script\n",
        "        # for dataset_name in dataset_names:\n",
        "    for data in datasets:\n",
        "        print(f\"\\nProcessing dataset: {data}\")\n",
        "        X, y = load_dataset(data)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "        n, d = X_train.shape\n",
        "        X_tensor= torch.tensor(X_test, dtype=torch.float32).to(device=device)\n",
        "        sampleNo_tbx = 200\n",
        "\n",
        "        load_models(data, X_tensor)\n",
        "        with open('feature_importances_new.pkl', 'rb') as f:\n",
        "            feature_importances = pickle.load(f)\n",
        "\n",
        "        # Plot feature importances for 10 randomly selected instances\n",
        "        # plot_feature_importances_matrix(feature_importances, method_names, num_instances=10)\n",
        "    ##-------------------\n",
        "    #2nd experiment\n",
        "        # Evaluate consistency across methods\n",
        "        n_clusters = 6  # Number of clusters to group similar instances\n",
        "        consistency_scores = consistency_across_methods(X_test, feature_importances, method_names, n_clusters)\n",
        "\n",
        "        # Output consistency scores for each method\n",
        "        print(\"\\nConsistency Scores (Lower is better):\")\n",
        "        for method_name, scores in consistency_scores.items():\n",
        "            print(f\"Method: {method_name}\")\n",
        "            for cluster_idx, score in enumerate(scores):\n",
        "                print(f\"  Cluster {cluster_idx + 1}: Consistency Score = {score:.20f}\")\n",
        "\n",
        "\n",
        "save_consistency_scores_to_csv(consistency_scores, data, filename=\"consistency_scores.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88-NxHURrsVE",
        "outputId": "5d142f41-7570-42ab-9343-6a4205de5062"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "\n",
            "Processing dataset: bike\n",
            "Applying HSICFeatureNetGumbelSparsemax on dataset: bike\n",
            "Applying HSICGumbelSparsemax on dataset: bike\n",
            "Evaluating consistency for method: Hsic_GumbelSparsemax...\n",
            "Evaluating consistency for method: HSICFeatureNet_GumbelSparsemax...\n",
            "\n",
            "Consistency Scores (Lower is better):\n",
            "Method: Hsic_GumbelSparsemax\n",
            "  Cluster 1: Consistency Score = 0.01594365786511106609\n",
            "  Cluster 2: Consistency Score = 0.00815244822348616671\n",
            "  Cluster 3: Consistency Score = 0.00533627678976368678\n",
            "  Cluster 4: Consistency Score = 0.00357485286410437909\n",
            "  Cluster 5: Consistency Score = 0.01103201116013217790\n",
            "  Cluster 6: Consistency Score = 0.01079698394507865207\n",
            "Method: HSICFeatureNet_GumbelSparsemax\n",
            "  Cluster 1: Consistency Score = 0.00081465835897546394\n",
            "  Cluster 2: Consistency Score = 0.00049837829331826958\n",
            "  Cluster 3: Consistency Score = 0.00046918501895984016\n",
            "  Cluster 4: Consistency Score = 0.00050685102759669099\n",
            "  Cluster 5: Consistency Score = 0.00133236433203584603\n",
            "  Cluster 6: Consistency Score = 0.00065882347480185810\n",
            "Consistency scores saved to consistency_scores.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCx7YkeKrsJe",
        "outputId": "63906aa9-a077-46bf-889e-148d646e63ac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "consistency_scores.csv\tfeature_importances_new.pkl  __pycache__       selected_features_new.pkl\n",
            "data\t\t\tHSICNet\t\t\t     real_datasets.py  trained_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the CSS file\n",
        "files.download('consistency_scores.csv')\n"
      ],
      "metadata": {
        "id": "_EbXGmal2Mbg",
        "outputId": "0da6ac2b-d920-4643-b270-7be9cfd35c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a6a47e89-4a5c-4abf-86ab-1d0e4b99030a\", \"consistency_scores.csv\", 694)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}